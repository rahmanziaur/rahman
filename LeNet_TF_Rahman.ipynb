{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMNNSrMoEUD32+yyNSGA8Qz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahmanziaur/rahman/blob/master/LeNet_TF_Rahman.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.contrib.layers import flatten\n",
        "from sklearn.utils import shuffle\n",
        "from tqdm import tqdm, trange\n",
        "\n",
        "\n",
        "class Lenet5():\n",
        "\n",
        "    def __init__(self, train_data, train_labels, test_data, test_labels, validation_data=None, validation_labels=None,\n",
        "                 mean=0, stddev=0.3, learning_rate=0.001):\n",
        "\n",
        "        self.train_data = train_data\n",
        "        self.train_labels = train_labels\n",
        "        assert (len(self.train_labels) == len(self.train_data))\n",
        "        assert (self.train_data[0].shape[0] == 32 and self.train_data[0].shape[1] == 32)\n",
        "\n",
        "        self.validation_data = validation_data\n",
        "        self.validation_labels = validation_labels\n",
        "        assert (len(self.validation_labels) == len(self.validation_data))\n",
        "        assert (self.validation_data[0].shape[0] == 32 and self.validation_data[0].shape[1] == 32)\n",
        "\n",
        "        self.test_data = test_data\n",
        "        self.test_labels = test_labels\n",
        "        assert (len(self.test_data) == len(self.test_labels))\n",
        "        assert (self.test_data[0].shape[0] == 32 and self.test_data[0].shape[1] == 32)\n",
        "\n",
        "        self.num_outputs = 10\n",
        "\n",
        "        self.X = tf.placeholder(tf.float32, shape=(None, 32, 32, 1))\n",
        "        self.y = tf.placeholder(tf.int32, shape=(None, self.num_outputs))\n",
        "\n",
        "        self.mu = mean\n",
        "        self.sigma = stddev\n",
        "\n",
        "        # Layer 1: Input 32x32x1, Output 28x28x6\n",
        "        self.conv1_kernels = tf.Variable(tf.truncated_normal(shape=[5, 5, 1, 6], mean=self.mu, stddev=self.sigma))\n",
        "        self.conv1_biases = tf.get_variable(name=\"conv1_biases\", shape=[6],\n",
        "                                            initializer=tf.random_normal_initializer(stddev=0.3))\n",
        "        self.conv1 = tf.nn.conv2d(self.X, self.conv1_kernels, [1, 1, 1, 1], padding='VALID') + self.conv1_biases\n",
        "        # Pooling -> from 28x28 to 14x14\n",
        "        self.pool1 = tf.nn.max_pool(self.conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
        "        # Activation\n",
        "        self.conv1 = tf.nn.relu(self.pool1)\n",
        "\n",
        "        # Layer 2: Input 14x14x6, Output 10x10x16\n",
        "        self.conv2_kernels = tf.Variable(tf.truncated_normal(shape=[5, 5, 6, 16], mean=self.mu, stddev=self.sigma))\n",
        "        self.conv2_biases = tf.get_variable(name=\"conv2_biases\", shape=[16],\n",
        "                                            initializer=tf.random_normal_initializer(stddev=self.sigma))\n",
        "        self.conv2 = tf.nn.conv2d(self.conv1, self.conv2_kernels, [1, 1, 1, 1], padding='VALID') + self.conv2_biases\n",
        "        # Pooling -> from 10x10x16 to 5x5x16\n",
        "        self.pool2 = tf.nn.max_pool(self.conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
        "        # Activation 2\n",
        "        self.conv2 = tf.nn.relu(self.pool2)\n",
        "\n",
        "        # Flatten -> from 5x5x16 to 400x1\n",
        "        self.flattened = flatten(self.conv2)\n",
        "\n",
        "        # Fully Connected Layer n.1\n",
        "        self.fcl1_weights = tf.Variable(tf.truncated_normal(shape=[400, 120], mean=self.mu, stddev=self.sigma))\n",
        "        self.fcl1_biases = tf.get_variable(name=\"fc1_biases\", shape=[120],\n",
        "                                           initializer=tf.random_normal_initializer(stddev=self.sigma))\n",
        "        self.fcl1 = tf.matmul(self.flattened, self.fcl1_weights) + self.fcl1_biases\n",
        "        # Activation 3\n",
        "        self.fcl1 = tf.nn.relu(self.fcl1)\n",
        "\n",
        "        # Fully Connected Layer n.2\n",
        "        self.fcl2_weights = tf.Variable(tf.truncated_normal(shape=[120, 84], mean=self.mu, stddev=self.sigma))\n",
        "        self.fcl2_biases = tf.get_variable(name=\"fc2_biases\", shape=[84],\n",
        "                                           initializer=tf.random_normal_initializer(stddev=self.sigma))\n",
        "        self.fcl2 = tf.matmul(self.fcl1, self.fcl2_weights) + self.fcl2_biases\n",
        "        # Activation 4\n",
        "        self.fcl2 = tf.nn.relu(self.fcl2)\n",
        "\n",
        "        # Fully Connected Layer n.3\n",
        "        self.fcl3_weights = tf.Variable(tf.truncated_normal(shape=[84, 10], mean=self.mu, stddev=self.sigma))\n",
        "        self.fcl3_biases = tf.get_variable(name=\"fc3_biases\", shape=[10],\n",
        "                                           initializer=tf.random_normal_initializer(stddev=self.sigma))\n",
        "        self.logits = tf.matmul(self.fcl2, self.fcl3_weights) + self.fcl3_biases\n",
        "\n",
        "        # Loss and metrics\n",
        "        self.cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(logits=self.logits, labels=self.y)\n",
        "        self.loss_op = tf.reduce_mean(self.cross_entropy)\n",
        "        self.optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "        self.training_step = self.optimizer.minimize(self.loss_op)\n",
        "\n",
        "        self.correct_prediction = tf.equal(tf.argmax(self.logits, 1), tf.argmax(self.y, 1))\n",
        "        self.accuracy_operation = tf.reduce_mean(tf.cast(self.correct_prediction, tf.float32))\n",
        "        self.saver = tf.train.Saver()\n",
        "\n",
        "\n",
        "    def train(self, epochs, batch_size, auto_save=True):\n",
        "        assert (epochs > 0 and batch_size > 0)\n",
        "\n",
        "        num_examples = len(self.train_data)\n",
        "\n",
        "        print('Training the model . . .')\n",
        "\n",
        "        with tf.Session() as session:\n",
        "            session.run(tf.global_variables_initializer())\n",
        "            total_steps = trange(epochs)\n",
        "            for epoch in total_steps:\n",
        "                self.train_data, self.train_labels = shuffle(self.train_data, self.train_labels)\n",
        "                for offset in range(0, num_examples, batch_size):\n",
        "                    end = offset + batch_size\n",
        "                    X_batch, y_batch = self.train_data[offset:end], self.train_labels[offset:end]\n",
        "\n",
        "                    _, acc, cross = session.run([self.training_step, self.accuracy_operation, self.cross_entropy],\n",
        "                                                feed_dict={self.X: X_batch, self.y: y_batch})\n",
        "\n",
        "                if self.validation_data != None:\n",
        "                    validation_accuracy = self.evaluate(self.validation_data, self.validation_labels, batch_size)\n",
        "                    total_steps.set_description(\n",
        "                        \"Epoch {} - validation accuracy {:.3f} \".format(epoch + 1, validation_accuracy))\n",
        "                    # print(\"Epoch {} - validation accuracy {:.3f} \".format(epoch+1,validation_accuracy))\n",
        "                    total_steps.set_description(\n",
        "                        \"Epoch {} - validation accuracy {:.3f} \".format(epoch + 1, validation_accuracy))\n",
        "\n",
        "                if auto_save and (epoch % 10 == 0):\n",
        "                    save_path = self.saver.save(session, 'tmp/model.ckpt'.format(epoch))\n",
        "\n",
        "            test_accuracy = self.evaluate(self.test_data, self.test_labels, batch_size=batch_size)\n",
        "            return test_accuracy\n",
        "\n",
        "    def evaluate(self, X_data, y_data, batch_size):\n",
        "        num_examples = len(X_data)\n",
        "        total_accuracy = 0\n",
        "        sess = tf.get_default_session()\n",
        "        for offset in range(0, num_examples, batch_size):\n",
        "            batch_x, batch_y = X_data[offset:offset + batch_size], y_data[offset:offset + batch_size]\n",
        "            accuracy = sess.run(self.accuracy_operation, feed_dict={self.X: batch_x, self.y: batch_y})\n",
        "            total_accuracy += (accuracy * len(batch_x))\n",
        "        return total_accuracy / num_examples\n",
        "\n",
        "    def restore_model(self, path):\n",
        "        with tf.Session() as session:\n",
        "            self.saver.restore(sess=session, save_path=path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387
        },
        "id": "uITc3OMHPabo",
        "outputId": "a2192785-d6e7-4149-9faf-1ac711a19298"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-6f76f63544f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mflatten\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrange\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.contrib'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387
        },
        "id": "j8fxmJsHKcp2",
        "outputId": "beb71ccf-c32b-497f-a40b-b3b0210064bf"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-8a401e3dc6ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtutorials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmnist\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minput_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlenet5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLenet5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# pd.read_csv('https://raw.githubusercontent.com/rahmanziaur/DTClassifierTest/main/TestDiabData_100Row.csv')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.examples'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "#from tensorflow.examples.tutorials.mnist import input_data\n",
        "import numpy as np\n",
        "from lenet5 import Lenet5\n",
        "\n",
        "# pd.read_csv('https://raw.githubusercontent.com/rahmanziaur/DTClassifierTest/main/TestDiabData_100Row.csv')\n",
        "mnist = input_data.read_data_sets(\"MNIST_data/\", reshape=False, one_hot=True)\n",
        "\n",
        "X_train, y_train = mnist.train.images, mnist.train.labels\n",
        "X_validation, y_validation = mnist.validation.images, mnist.validation.labels\n",
        "X_test, y_test = mnist.test.images, mnist.test.labels\n",
        "\n",
        "print(\"Input shape: {}\".format(X_train[0].shape))\n",
        "print(\"Training Set:   {} samples\".format(len(X_train)))\n",
        "print(\"Validation Set: {} samples\".format(len(X_validation)))\n",
        "print(\"Test Set:       {} samples\".format(len(X_test)))\n",
        "\n",
        "# 0-Padding for LeNet-5's input size\n",
        "X_train = np.pad(X_train, ((0, 0), (2, 2), (2, 2), (0, 0)), 'constant')\n",
        "X_validation = np.pad(X_validation, ((0, 0), (2, 2), (2, 2), (0, 0)), 'constant')\n",
        "X_test = np.pad(X_test, ((0, 0), (2, 2), (2, 2), (0, 0)), 'constant')\n",
        "\n",
        "print(\"New Input shape: {}\".format(X_train[0].shape))\n",
        "\n",
        "lenet_network = Lenet5(X_train,y_train,X_test,y_test,X_validation,y_validation)\n",
        "accuracy = lenet_network.train(epochs=10,batch_size=100)\n",
        "print(\"Accuracy on test set: {:.3f}\".format(accuracy))\n",
        "\n",
        "'''\n",
        "# TODO: some refactoring for restoring the model\n",
        "lenet_network_restored = Lenet5(X_train, y_train, X_test, y_test, X_validation, y_validation)\n",
        "lenet_network_restored.restore_model(path='tmp/model.ckpt')\n",
        "'''"
      ]
    }
  ]
}